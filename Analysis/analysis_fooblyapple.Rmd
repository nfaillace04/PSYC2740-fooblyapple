---
title: "analysis_fooblyapple"
ggeom_point(date: "2024-11-12"
---
# Install and load packages
```{r}
#install.packages("tidyverse")
#install.packages("emmeans")
#install.packages("car")
#install.packages("sentimentr")
#install.packages("lmerTest")
#install.packages("performance")

library(tidyverse)
library(emmeans)
library(car)
library(sentimentr)
library(lmerTest)
library(performance)
```
# Import data
```{r}
sona_data = read.csv("../data/savic-valence.csv") %>%
  select(-sona_id) %>% 
  mutate(data_source = "sona")

prolific_data = read.csv("../data/prolificdatanov22.csv")%>%
  select(-PROLIFIC_PID) %>% 
  mutate(data_source = "prolific")

final_data = rbind(sona_data, prolific_data) %>%
  mutate(rt = as.numeric(rt)) %>%
  mutate(valence = as.factor(valence)) %>%
  mutate(typeoftrial = as.factor(typeoftrial))

```
# Inspect data
```{r}
nrow(final_data)
ncol(final_data)

final_data %>%
pull(ID) %>% unique() %>% length

final_data %>%
  group_by(ID) %>% count()

final_data %>%
  group_by(ID) %>% filter(typeoftrial == "target") %>% count()

final_data %>% 
  group_by(ID) %>% filter(typeoftrial == "target" & block_number == 1)%>% count()

final_data %>%
  group_by(ID) %>% filter(typeoftrial == "sentence") %>% count() 

final_data %>%
  group_by(ID) %>% filter(typeoftrial == "attention") %>% count() 

```
There are 86680 rows and 43 columns in our data, and 166 unique subjects. All subjects that completed the experiment completed 105 target trials (9 practice and 96 experimental), 126 sentence trials, and 9 attention trials. All subjects did the same number of trials because unlike in the demographics survey, the number of trials should not change based on the responses of the subject.

Our independent variables are "valence" (positive, negative, neutral) and "type" (novel or familiar). Our dependent variables are "rt" for priming trials.

# Basic descriptives
```{r}
demographics = final_data %>%
 filter(typeoftrial == "demographics") %>%
  select(ID, age, gender, education, race, hispanic, dominant_hand, alert_time, english, age_learned_english) %>%
  mutate(across(c(age, education, age_learned_english), ~ replace_na(., NA))) %>%
  mutate(across(c(age, gender, education, race, hispanic, dominant_hand, alert_time, english), 
                ~ ifelse(. == "", "blank", .)))

subject_age = demographics %>%
summarise(mean_age = mean(age, na.rm = TRUE),
            sd_age = sd(age, na.rm = TRUE))

gender_distribution = demographics %>%
  filter(gender != "blank") %>%
  mutate(gender_cleaned = case_when(
    str_trim(gender) %in% c("Female", "female", "F", "FEMALE", "Female.", "Female/woman", "cis-women") ~ "Female",
    str_trim(gender) %in% c("Male", "male", "m", "M") ~ "Male",
    TRUE ~ "Other"
  )) %>%
  count(gender_cleaned)

race_distribution = demographics %>%
  filter(race != "blank") %>%
  count(race)

subject_education = demographics %>%
  summarise(mean_education = mean(education, na.rm = TRUE),
            sd_education = sd(education, na.rm = TRUE))

target_accuracy = final_data %>%
  filter(typeoftrial == "target") %>%
  group_by(ID) %>%
  summarise(correct_count = sum(correct == TRUE, na.rm = TRUE),
    total_count = n(),                                    
    accuracy = correct_count / total_count)

mean_target_accuracy = target_accuracy %>%
  summarise(mean_accuracy = mean(accuracy),
            sd_accuracy = sd(accuracy))
 
final_data %>%
  filter(typeoftrial == "target") %>%
  filter(rt < 1500) %>%
ggplot()+
  geom_histogram(mapping = aes (x = rt), binwidth = 30, na.rm = TRUE)+
 labs(title = "Histogram of Response Time",
       x = "Response Time",
       y = "Count") +
  theme_classic()
```
Based on the histogram, response times can be represented by a positively-skewed distribution with a peak around 500ms. The tail seems to taper around 1500ms, which we established as a cut-off.
```{r}
attention = final_data %>%
  filter(typeoftrial == "attention") %>%
  select(ID, response, novel, correct) %>%
  rowwise() %>%
  mutate(response = ifelse(is.na(response), "blank", response)) %>%
mutate(across(c(novel), ~ replace_na(., "NOT_FOUND"))) %>%
  mutate(edit_novel = adist(novel, response))%>%
  mutate(revised_correct = ifelse (edit_novel < 3, 1, 0),
         mismatch = ifelse (correct == revised_correct, 0, 1)) %>%
  ungroup()

subject_attention_accuracy = attention %>%
  group_by(ID) %>%
  summarise(mean_accuracy = mean(revised_correct, na.rm = TRUE),
            sd_accuracy = sd(revised_correct, na.rm = TRUE))

```
The mean age of our sample is 33.91, and the standard deviation is 13.32. The gender distribution is currently 84 females, 60 males, and 3 participants who identified as nonbinary. In terms of the racial distribution, there is 1 subject who identified as American Indian/Alaskan Native, 12 as Asian, 24 as Black/African American, 92 as White/Caucasian, 9 as multiracial, and 9 as other. The mean number of years of education in our sample was 15.07, with a standard deviation of 3.28.

The average accuracy in the target trials was 0.71, with a standard deviation of 0.45. 


# Inferential statistics
Primary research question: To what extent does word valence impact word recognition? Operationalized by response times.
```{r}
low_acc_IDs = subject_attention_accuracy %>%
   filter(mean_accuracy < 0.75) %>%
   select(ID, mean_accuracy)

low_acc_IDs_vector = low_acc_IDs$ID %>%
  length()

revised_critical_data = final_data %>%
  select(ID, rt, condition, prime, correct, target, correct_key, block_number, typeoftrial) %>%
  filter(typeoftrial == "target", block_number == 1) %>%
  filter(!is.na(rt), rt > 200, rt < 1500, correct %in% c("true", "TRUE")) %>%
  anti_join(low_acc_IDs, by = "ID")

included_in_priming = revised_critical_data %>%
  pull(ID) %>%
  unique() %>%
  length()

valence_codes = read_csv("../data/valence_codes.csv")

revised_critical_data = revised_critical_data %>% 
  filter(typeoftrial == "target" & block_number == 1)%>%
  select(ID, condition, rt, prime, target, correct) %>%
  left_join(valence_codes) %>% # for foobly, dodish, nuppical, mipp
  filter(!(prime %in% c("boff", "geck")))

revised_critical_data %>%
  group_by(ID) %>% count()

counts = revised_critical_data %>%
  group_by(valence) %>%
  count()
```
Exclusions: The number of participants initially recruited was 166. We excluded 57 participants based on pre-registration criteria, and another 19 for not completing all parts of the experiment. In our models, we analyzed the data from 90 total participants. 
```{r}
mean_scores = revised_critical_data %>%
  group_by(valence) %>%
  summarize(mean_rt = mean(rt),
           sd_rt = sd(rt)) %>%
  left_join(counts) %>%
  mutate(SE = sd_rt/sqrt(n),
         ymin = mean_rt - 1.96*SE,
         ymax = mean_rt + 1.96*SE)
  
mean_scores %>%
  group_by(valence) %>%
    ggplot() +
    geom_col(mapping = aes(x = valence, y = mean_rt,
                           group = valence, fill = valence),
             position = "dodge")+
  geom_errorbar(aes(x = valence, ymin = ymin, ymax = ymax),
                width = .25,
                position = position_dodge(width=0.9)) + 
  geom_point(data = mean_scores, aes(x = valence, y=mean_rt, colour = valence),
             position = position_jitterdodge(),
             alpha = 0.3)+
  labs(title = "Mean Response Time Based on Prime Valence",
       x = "Prime Valence",
       y = "Mean RT") +
    theme_classic()
```
Based on the bar graph, the mean_rt for all valences appear to be equal.
```{r}
rt_model = lmer(data = revised_critical_data,
                rt ~ valence + (1|ID))
car::Anova(rt_model)
nobs(rt_model)
```
Our statistical tests indicated that there was no main effect of valence on reaction time, χ2(3, N = 4942) = 1.884, *p* = .597.
```{r}
meaning_check = final_data %>%
  filter(typeoftrial == "meaning_check") %>%
  select(ID, condition, response, cue) %>%
  filter(str_count(response, "\\S+") >= 3)
 
meaning_check_correct = meaning_check %>%
  mutate(prime = str_extract(cue, "\\S+$"),
    correct = if_else(str_detect(str_to_lower(response), str_to_lower(prime)), "TRUE", "FALSE"))

revised_meaning_check = meaning_check_correct %>%
  filter(correct == "TRUE")
  
#meaning_check_sentiment
mc_sentiment_score = sentiment(revised_meaning_check$response)

aggregated_scores = mc_sentiment_score %>%
  group_by(element_id) %>% 
  summarise(sentiment = mean(sentiment),
    word_count = sum(word_count))

meaning_check_sentiment = revised_meaning_check %>%
  mutate(element_id = row_number()) %>%
  left_join(aggregated_scores, by = "element_id") %>% 
  select(-element_id, -word_count) %>%
  left_join(valence_codes, by = c("prime", "condition"))

meaning_check_sentiment %>%
  ggplot(aes(x = valence, y = sentiment)) +
  geom_boxplot() +
  theme_linedraw() +
  labs(title = "Meaning Check Sentiment Compared to Intended Valence", x = "Valence", y = "Sentiment")

meaning_check_model = lmer(data = meaning_check_sentiment,
                sentiment ~ valence + (1|ID))
car::Anova(meaning_check_model)
nobs(meaning_check_model)
emmeans::emmeans(meaning_check_model, pairwise ~ valence, adjust = "tukey")
```
Our testing suggested there was a main effect of valence on sentiment score, (χ2(2, N = 856) = 122.56, *p* < .001).

Follow-up Tukey p adjustment tests for pairwise comparison indicate that valence of the priming sentences was significantly predictive of sentiment score for sentences created during meaning check. All pairwise comparisons confirm that these differences are statistically significant (*p*'s < .001). We can conclude that our subjects were largely able to accurately associate novel words with their conditional valence.
# Sentence boxplot
```{r}
directory = "../Experiment"

all_csv_files = list.files(directory, pattern = "^valence_sentences.*-updated\\.csv$", full.names = TRUE)

sentences = all_csv_files %>%
  purrr::map_dfr(read_csv) %>%
  mutate(element_id = row_number()) %>%
  select(-part)

sentence_sentiment_score = sentiment(sentences$sentence)

aggregated_sentence_scores = sentence_sentiment_score %>%
  group_by(element_id) %>% 
  summarise(sentiment = mean(sentiment),
    word_count = sum(word_count))

sentences = sentences %>%
  left_join(aggregated_sentence_scores, by = "element_id") %>%
  select(-word_count)

sentences %>%
  ggplot(aes(x = valence, y = sentiment)) +
  geom_boxplot() +
  theme_linedraw() +
  labs(title = "Sentence Sentiment Compared to Intended Valence", x = "Valence", y = "Sentiment")
```
This boxplot represents the sentiment scores of the sentences used in training and a visual reflection of how our valence groups correlated with sentiment scores. Notably, there is an outlier in the negative valence. This sentence is "I hope they do not have a nuppical apple" which received a sentiment score of 0.16667. This outlier reflects a limitation of sentimentr: a sentence that would be interpreted as negative by a human subject is recorded as positive.

