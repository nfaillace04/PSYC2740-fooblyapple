---
title: "analysis_fooblyapple"
ggeom_point(date: "2024-11-12"
---
#install and load packages
```{r}
#install.packages("tidyverse")
#install.packages("emmeans")
#install.packages("car")
#install.packages("sentimentr")
#install.packages("lmerTest")

library(tidyverse)
library(emmeans)
library(car)
library(sentimentr)
library(lmerTest)
```
#Import data
```{r}
sona_data = read.csv("../data/recentsonadata.csv") %>%
  select(-sona_id) %>% 
  mutate(data_source = "sona")

prolific_data = read.csv("../data/savic-valence-prolific.csv")%>%
  select(-PROLIFIC_PID) %>% 
  mutate(data_source = "prolific")

final_data = rbind(sona_data, prolific_data)

```
#Inspect data
```{r}
nrow(final_data)
ncol(final_data)

final_data %>%
pull(ID) %>% unique() %>% length

final_data %>%
  group_by(ID) %>% count()

final_data %>% 
  filter(typeoftrial == "target" & block_number == 1)%>%
  group_by(ID) %>%
  mutate(rt = as.numeric(rt)) %>%
  count()

as.factor(final_data$valence)
as.factor(final_data$type)

final_data %>%
  group_by(ID) %>% filter(typeoftrial == "target") %>% count()

final_data %>%
  group_by(ID) %>% filter(typeoftrial == "sentence") %>% count() 

final_data %>%
  group_by(ID) %>% filter(typeoftrial == "attention") %>% count() 

```
There are 59740 rows and 43 columns in our data, and 108 unique subjects. They each completed 105 target trials (9 practice and 96 experimental), 126 sentence trials, and 9 attention trials. All subjects did the same number of trials because unlike in the demographics survey, the number of trials should not change based on the responses of the subject.

Our independent variables are "valence" (positive, negative, neutral) and "type" (novel or familiar). Our dependent variables are "rt" for priming trials.

#Basic descriptives
```{r}
demographics = final_data %>%
 filter(typeoftrial == "demographics") %>%
  select(ID, age, gender, education, race, hispanic, dominant_hand, alert_time, english, age_learned_english) %>%
  mutate(across(c(age, education, age_learned_english), ~ replace_na(., NA))) %>%
  mutate(across(c(age, gender, education, race, hispanic, dominant_hand, alert_time, english), 
                ~ ifelse(. == "", "blank", .)))

subject_age = demographics %>%
summarise(mean_age = mean(age, na.rm = TRUE),
            sd_age = sd(age, na.rm = TRUE))

gender_distribution = demographics %>%
  filter(gender != "blank") %>%
  mutate(gender_cleaned = case_when(
      str_trim(gender) %in% c("Female", "female", "F", "Female.", "Female/woman", "cis-women") ~ "Female",
      str_trim(gender) %in% c("Male", "male", "M") ~ "Male",
      TRUE ~ "Other"
    )
  ) %>%
  count(gender_cleaned)

race_distribution = demographics %>%
  filter(race != "blank") %>%
  count(race)

subject_education = demographics %>%
  summarise(mean_education = mean(education, na.rm = TRUE),
            sd_education = sd(education, na.rm = TRUE))

target_accuracy = final_data %>%
  filter(typeoftrial == "target") %>%
  group_by(ID) %>%
  summarise(correct_count = sum(correct == TRUE, na.rm = TRUE),
    total_count = n(),                                    
    accuracy = correct_count / total_count)

mean_target_accuracy = target_accuracy %>%
  summarise(mean_accuracy = mean(accuracy),
            sd_accuracy = sd(accuracy))

target_data = final_data %>%
  filter(typeoftrial == "target") %>%
  group_by(ID)%>%
  mutate(rt = as.numeric(rt))

response_times = target_data %>%
  select(ID, rt) %>%
  group_by(ID) %>%
  summarise(mean_rt = mean(as.numeric(rt)), sd_rt = sd(rt))

ggplot(data = response_times)+
  geom_histogram(mapping = aes (x = mean_rt), binwidth = 30, na.rm = TRUE)+
 labs(title = "Histogram of Mean Response Time",
       x = "Mean Response Time",
       y = "Count") +
   xlim(0, 1500) +
  ylim(0, 20)
  theme_classic()
range(as.numeric(target_data$rt))

attention = final_data %>%
  filter(typeoftrial == "attention") %>%
  select(ID, response, novel, correct) %>%
  rowwise() %>%
  mutate(response = ifelse(is.na(response), "blank", response)) %>%
mutate(across(c(novel), ~ replace_na(., "NOT_FOUND"))) %>%
  mutate(edit_novel = adist(novel, response))%>%
  mutate(revised_correct = ifelse (edit_novel < 3, 1, 0), #changed to three letters difference as criterion for correct
         mismatch = ifelse (correct == revised_correct, 0, 1)) %>%
  ungroup()

meaning_check = final_data %>%
  filter(typeoftrial == "meaning_check") %>%
  select(ID, condition, response, cue) %>% #encountered that we do not have the target stimulus word saved anywhere, so not sure how to determine what is correct.
  filter(str_count(response, "\\S+") >= 3)

subject_attention_accuracy = attention %>%
  group_by(ID) %>%
  summarise(mean_accuracy = mean(revised_correct, na.rm = TRUE),
            sd_accuracy = sd(revised_correct, na.rm = TRUE))

```
The mean age of our sample is 35.38, and the standard deviation is 13.18. The gender distribution is currently 65 females and 37 males. In terms of the racial distribution, there is 1 subject who identified as American Indian/Alaskan Native, 8 as Asian, 13 as Black/African American, 70 as White/Caucasian, 5 as multiracial, and 5 as other. The mean number of years of education in our sample was 15.16, with a standard deviation of 3.06.

The average accuracy in the target trials was 0.88, with a standard deviation of 0.13. 


#Inferential Statistics
Primary research question: To what extent does word valence impact word recognition? Operationalized by response times.
```{r}
low_acc_IDs = subject_attention_accuracy %>%
  filter(mean_accuracy < 0.75) %>%
  pull(ID)

revised_critical_data = target_data %>%
  filter(block_number == 1) %>%
  select(ID, rt, condition, prime, correct, target, correct_key, block_number) %>%
  filter(!is.na(rt), rt > 200, rt < 1500, correct == TRUE) %>%
  filter(!ID %in% low_acc_IDs)
```
# priming_valence_codes
```{r}
valence_codes = read_csv("../data/valence_codes.csv")

priming_data = target_data %>% 
  filter(typeoftrial == "target" & block_number == 1)%>%
  select(condition, rt, prime, target, correct) %>%
  left_join(valence_codes) %>% # for foobly, dodish, nuppical, mipp
  filter(!(prime %in% c("boff", "geck")))

priming_data %>%
pull(ID) %>% unique() %>% length

priming_data %>%
  group_by(ID) %>% count()

priming_data %>%
  group_by(valence) %>%
    summarize(mean_rt = mean(rt)) %>%
    ggplot() +
    geom_col(mapping = aes(x = valence, y = mean_rt,
                           group = valence, fill = valence),
             position = "dodge")+
    theme_classic()

rt_model = lmer(data = priming_data,
                rt ~ valence + (1|ID))
emmeans::emmeans(rt_model,
                 pairwise ~ valence,
                 adjust = "tukey")
 
meaning_check_correct = meaning_check %>%
  mutate(prime = str_extract(cue, "\\S+$"),
    correct = if_else(str_detect(str_to_lower(response), str_to_lower(prime)), "TRUE", "FALSE"))

revised_meaning_check = meaning_check_correct %>%
  filter(correct == "TRUE")
  
#meaning_check_sentiment
mc_sentiment_score = sentiment(revised_meaning_check$response)

aggregated_scores = mc_sentiment_score %>%
  group_by(element_id) %>% 
  summarise(sentiment = mean(sentiment),
    word_count = sum(word_count))

meaning_check_sentiment = revised_meaning_check %>%
  mutate(element_id = row_number()) %>%
  left_join(aggregated_scores, by = "element_id") %>% 
  select(-element_id, -word_count) %>%
  left_join(valence_codes, by = c("prime", "condition"))

meaning_check_sentiment %>%
  ggplot(aes(x = valence, y = sentiment)) +
  geom_boxplot() +
  theme_linedraw() +
  labs(title = "Boxplot of Meaning Check Sentiment Compared to Intended Valence", x = "Valence", y = "Sentiment")

meaning_check_model = lmer(data = meaning_check_sentiment,
                sentiment ~ valence + (1|ID))
emmeans::emmeans(meaning_check_model,
                 pairwise ~ valence,
                 adjust = "tukey")
```
Based on the bar graph, the positive mean_rt is higher than all the other valence levels. The unrelated valence seems to be only slightly lower than the rest and mean_rt for negative and neutral conditions appear comparable.

Follow-up tests indicate that there is no significant difference between response times in any condition. There is no comparison where (p < 0.001). These findings suggest that valence does not have an impact on reaction time in this model.

The positive valence is associated with the highest outcome, t(456) = 0.1330, followed by neutral, t(458) = 0.0326, and negative valence is associated with the lowest outcome, t(455) = -0.0542. All pairwise comparisons confirm that these differences are statistically significant (p < 0.001). We can conclude that people generally did accurately associate novel words with their conditonal valence.

#Sentence boxplot
```{r}
directory = "../Experiment"

# Get a list of CSV files that match the pattern
all_csv_files = list.files(directory, pattern = "^valence_sentences.*-updated\\.csv$", full.names = TRUE)

# Read and combine all CSV files into one data frame called 'sentences'
sentences = all_csv_files %>%
  purrr::map_dfr(read_csv) %>%
  mutate(element_id = row_number()) %>%
  select(-part)

sentence_sentiment_score = sentiment(sentences$sentence)

aggregated_sentence_scores = sentence_sentiment_score %>%
  group_by(element_id) %>% 
  summarise(sentiment = mean(sentiment),
    word_count = sum(word_count))

sentences = sentences %>%
  left_join(aggregated_sentence_scores, by = "element_id") %>%
  select(-word_count)

sentences %>%
  ggplot(aes(x = valence, y = sentiment)) +
  geom_boxplot() +
  theme_linedraw() +
  labs(title = "Boxplot of Sentence Sentiment", x = "Valence", y = "Sentiment")
```


