---
title: "analysis_fooblyapple"
ggeom_point(date: "2024-11-12"
---
#install and load packages
```{r}
#install.packages("tidyverse")
#install.packages("emmeans")
#install.packages("car")
#install.packages("sentimentr")
#install.packages("lmerTest")

library(tidyverse)
library(emmeans)
library(car)
library(sentimentr)
library(lmerTest)
```
#Import data
```{r}
sona_data = read.csv("../data/recentsonadata.csv") %>%
  select(-sona_id) %>% 
  mutate(data_source = "sona")

prolific_data = read.csv("../data/savic-valence-prolific.csv")%>%
  select(-PROLIFIC_PID) %>% 
  mutate(data_source = "prolific")

final_data = rbind(sona_data, prolific_data)

```
#Inspect data
```{r}
nrow(final_data)
ncol(final_data)

final_data %>%
pull(ID) %>% unique() %>% length

final_data %>%
  group_by(ID) %>% count()

final_data %>% 
  filter(typeoftrial == "target" & block_number == 1)%>%
  group_by(ID) %>%
  mutate(rt = as.numeric(rt)) %>%
  count()
#There are 59740 rows and 43 columns in our data, and 108 unique subjects. They each completed 105 target trials (9 practice and 96 experimental), 126 sentence trials, and 9 attention trials. All subjects did the same number of trials because unlike in the demographics survey, the number of trials should not change based on the responses of the subject.

#Our independent variables are "valence" (positive, negative, neutral) and "type" (novel or familiar). Our dependent variables are "rt" for priming trials.

as.factor(final_data$valence)
as.factor(final_data$type)

final_data %>%
  group_by(ID) %>% filter(typeoftrial == "target") %>% count()

final_data %>%
  group_by(ID) %>% filter(typeoftrial == "sentence") %>% count() 

final_data %>%
  group_by(ID) %>% filter(typeoftrial == "attention") %>% count() 

```
If number of target trials is not 105, exclude from data. If meaningchecks don't equal 6, exclude.

#Basic descriptives
```{r}
demographics = final_data %>%
 filter(typeoftrial == "demographics") %>%
  select(ID, age, gender, education, race, hispanic, dominant_hand, alert_time, english, age_learned_english) %>%
  mutate(across(c(age, education, age_learned_english), ~ replace_na(., NA))) %>%
  mutate(across(c(age, gender, education, race, hispanic, dominant_hand, alert_time, english), 
                ~ ifelse(. == "", "blank", .)))

subject_age = demographics %>%
summarise(mean_age = mean(age, na.rm = TRUE),
            sd_age = sd(age, na.rm = TRUE))

gender_distribution = demographics %>%
  filter(gender != "blank") %>%
  count(gender)

race_distribution = demographics %>%
  filter(race != "blank") %>%
  count(race)

subject_education = demographics %>%
  summarise(mean_education = mean(education, na.rm = TRUE),
            sd_education = sd(education, na.rm = TRUE))

target_accuracy = final_data %>%
  filter(typeoftrial == "target") %>%
  group_by(ID) %>%
  summarise(correct_count = sum(correct == TRUE, na.rm = TRUE),
    total_count = n(),                                    
    accuracy = correct_count / total_count)

mean_target_accuracy = target_accuracy %>%
  summarise(mean_accuracy = mean(accuracy),
            sd_accuracy = sd(accuracy))

target_data = final_data %>%
  filter(typeoftrial == "target") %>%
  group_by(ID)%>%
  mutate(rt = as.numeric(rt))

response_times = target_data %>%
  select(ID, rt) %>%
  group_by(ID) %>%
  summarise(mean_rt = mean(as.numeric(rt)))

ggplot(data = response_times)+
  geom_histogram(mapping = aes (x = mean_rt), binwidth = 30, na.rm = TRUE)+
 labs(title = "Histogram of Mean Response Time",
       x = "Mean Response Time",
       y = "Count") +
   xlim(0, 1500) +
  ylim(0, 20)
  theme_classic()
range(as.numeric(target_data$rt))

attention = final_data %>%
  filter(typeoftrial == "attention") %>%
  select(ID, response, novel, correct) %>%
  rowwise() %>%
  mutate(response = ifelse(is.na(response), "blank", response)) %>%
mutate(across(c(novel), ~ replace_na(., "NOT_FOUND"))) %>%
  mutate(edit_novel = adist(novel, response))%>%
  mutate(revised_correct = ifelse (edit_novel < 3, 1, 0), #changed to three letters difference as criterion for correct
         mismatch = ifelse (correct == revised_correct, 0, 1)) %>%
  ungroup()

meaning_check = final_data %>%
  filter(typeoftrial == "meaning_check") %>%
  select(ID, condition, response, cue) %>% #encountered that we do not have the target stimulus word saved anywhere, so not sure how to determine what is correct.
  filter(str_count(response, "\\S+") >= 3)

subject_attention_accuracy = attention %>%
  group_by(ID) %>%
  summarise(mean_accuracy = mean(revised_correct, na.rm = TRUE),
            sd_accuracy = sd(revised_correct, na.rm = TRUE))

```
#Inferential Statistics
Primary research question: To what extent does word valence impact word recognition? Operationalized by response times.
```{r}
low_acc_IDs = subject_attention_accuracy %>%
  filter(mean_accuracy < 0.75) %>%
  pull(ID)

revised_critical_data = target_data %>%
  filter(block_number == 1) %>%
  select(ID, rt, condition, prime, correct, target, correct_key, block_number) %>%
  filter(!is.na(rt), rt > 200, rt < 1500, correct == TRUE) %>%
  filter(!ID %in% low_acc_IDs)
```
# priming_valence_codes
```{r}
valence_codes = read_csv("../data/valence_codes.csv")

priming_data = target_data %>% 
  filter(typeoftrial == "target" & block_number == 1)%>%
  select(condition, rt, prime, target, correct) %>%
  left_join(valence_codes) %>% # for foobly, dodish, nuppical, mipp
  filter(!(prime %in% c("boff", "geck")))

priming_data %>%
  group_by(valence) %>%
    summarize(mean_rt = mean(rt)) %>%
    ggplot() +
    geom_col(mapping = aes(x = valence, y = mean_rt,
                           group = valence, fill = valence),
             position = "dodge")+
    theme_classic()
#Based on the bar graph, the positive mean_rt is higher than all the other valence levels. The unrelated valence seems to be only slightly lower than the rest and mean_rt for negative and neutral conditions appear comparable.

rt_model = lmer(data = priming_data,
                rt ~ valence + (1|ID))
emmeans::emmeans(rt_model,
                 pairwise ~ valence,
                 adjust = "tukey")

#Follow-up tests indicate that there is no significant difference between response times in any condition. There is no comparison where (p < 0.001). These findings suggest that valence does not have an impact on reaction time in this model.
 
meaning_check_correct = meaning_check %>%
  mutate(prime = str_extract(cue, "\\S+$"),
    correct = if_else(str_detect(str_to_lower(response), str_to_lower(prime)), "TRUE", "FALSE"))

revised_meaning_check = meaning_check_correct %>%
  filter(correct == "TRUE")
  
#meaning_check_sentiment
mc_sentiment_score = sentiment(revised_meaning_check$response)

aggregated_scores = mc_sentiment_score %>%
  group_by(element_id) %>% 
  summarise(sentiment = mean(sentiment),
    word_count = sum(word_count))

meaning_check_sentiment <- revised_meaning_check %>%
  mutate(element_id = row_number()) %>%
  left_join(aggregated_scores, by = "element_id") %>% 
  select(-element_id, -word_count) %>%
  left_join(valence_codes, by = c("prime", "condition"))
```


